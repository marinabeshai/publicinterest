{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 3: PROFILE + GEN Q'S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.csv_utils as csv_utils \n",
    "import utils.dir_utils as dir_utils\n",
    "import utils.dict_utils as dict_utils \n",
    "import utils.ptr_utils as ptr_utils\n",
    "import utils.constants as constants \n",
    "import helpers.official as official\n",
    "import helpers.search as search\n",
    "import helpers.congress as congress\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, input_df = dir_utils.get_data(combined=True)\n",
    "    \n",
    "sector_df = dir_utils.get_mapping(sector=True)\n",
    "industry_df = dir_utils.get_mapping(industry=True)\n",
    "\n",
    "# {canonical_name_input_based : link, ...}\n",
    "input_all_officials_name = {}\n",
    "# {link : canonical_name_input_based, ....} \n",
    "input_all_officials_link = {} \n",
    "# (canonical_name_input_based, ...)\n",
    "names = set()\n",
    "\n",
    "for _,t in input_df.iterrows():        \n",
    "    name = official.get_name(t)\n",
    "    if name not in names:    \n",
    "        link = search.get_wiki_link(name)        \n",
    "        input_all_officials_link = dict_utils.increment_dictionary(input_all_officials_link, link, name, not_math=True)\n",
    "        input_all_officials_name = dict_utils.increment_dictionary(input_all_officials_name, name, link, not_math=True)\n",
    "        names.add(name)\n",
    "\n",
    "# {link : (canonical_name_input_based, official_object), ... }\n",
    "input_all_officials_objects = {}\n",
    "for link, person in input_all_officials_link.items(): \n",
    "    off = search.wiki_search(person)      \n",
    "    input_all_officials_objects[link] = (person, off)\n",
    "\n",
    "# 17m 42.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero = pd.read_csv(\"../results/csv/profit/sample/b_0.csv\")\n",
    "# seven = pd.read_csv(\"../results/csv/profit/sample/b_7.csv\")\n",
    "# fourteen = pd.read_csv(\"../results/csv/profit/sample/b_14.csv\")\n",
    "\n",
    "# # copying people from certain committees?\n",
    "\n",
    "# def t_to_obj(t):\n",
    "#     name = t['name']\n",
    "#     link = input_all_officials_name[name]\n",
    "#     _, obj = input_all_officials_objects[link]\n",
    "#     return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def mean1(d):\n",
    "    for k,v in d.items():\n",
    "        if v != 0 and v != []:\n",
    "            # print(v)\n",
    "            # print(numpy.average(v))\n",
    "            d[k] = round(float(numpy.average(v)), 2)\n",
    "        \n",
    "    return d \n",
    "\n",
    "\n",
    "def z(group):\n",
    "    p26 = {}\n",
    "    p52 = {}\n",
    "    p104 = {}\n",
    "\n",
    "    for _, t in group.iterrows():\n",
    "        obj = t_to_obj(t)\n",
    "        # committees = obj.get_asgts()\n",
    "        sector = dir_utils.search_mapping(sector_df, t['ticker_purch'], sector=True)\n",
    "        # for i in committees:\n",
    "        #     i = i [ : i.find(\"(\")]\n",
    "        #     if 'Subcommittee' not in i:\n",
    "        if ptr_utils.isvalid(t['p_104']): \n",
    "            p104 = dict_utils.increment_list_in_dictionary(p104, sector, float(t['p_104']))\n",
    "        if  ptr_utils.isvalid(t['p_52']): \n",
    "            p52 = dict_utils.increment_list_in_dictionary(p52, sector, float(t['p_52']))\n",
    "        if  ptr_utils.isvalid(t['p_26']): \n",
    "            p26 = dict_utils.increment_list_in_dictionary(p26, sector, float(t['p_26']))\n",
    "         \n",
    "    p26 = mean1(p26)\n",
    "    p52 = mean1(p52) \n",
    "    p104 = mean1(p104)\n",
    "            \n",
    "    return p26, p52, p104\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p26, p52, p104 = z(zero)\n",
    "csv_utils.make_csv_multiple_dicts('/', \"b_0_sector\", (p26, p52, p104), ['', 'p26', 'p52', 'p104'])\n",
    "p26, p52, p104 = z(seven)\n",
    "csv_utils.make_csv_multiple_dicts('/', \"b_7_sector\", (p26, p52, p104), ['', 'p26', 'p52', 'p104'])\n",
    "p26, p52, p104 = z(fourteen)\n",
    "csv_utils.make_csv_multiple_dicts('/', \"b_14_sector\", (p26, p52, p104), ['', 'p26', 'p52', 'p104'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {link : (canonical_name_input_based, official_object), ... }\n",
    "questionable_people_objects = {}\n",
    "indexes_to_keep = [] \n",
    "\n",
    "for index,t in input_df.iterrows():\n",
    "    if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "        name = official.get_name(t)\n",
    "        year = str(ptr_utils.get_year((t[constants.TDATE])))\n",
    "        sector = dir_utils.search_mapping(sector_df, t[constants.TICKER], sector=True)\n",
    "        obj = t_to_obj(t)\n",
    "        committees = obj.get_asgts()\n",
    "        \n",
    "        for comm in committees:\n",
    "            comm_year = official.get_committee_year(comm)                \n",
    "            if sector in comm and year in comm_year: \n",
    "                link = input_all_officials_name[name]\n",
    "                questionable_people_objects[link] =  (name, obj)\n",
    "                indexes_to_keep.append(index)\n",
    "                break\n",
    "        \n",
    "        if index not in indexes_to_keep:\n",
    "            input_df.drop(index, inplace=True)\n",
    "    else:\n",
    "        input_df.drop(index, inplace=True)\n",
    "\n",
    "input_df.to_csv(\"questionable.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = set()\n",
    "# i = 0 \n",
    "# for _,t in input_df.iterrows():\n",
    "#     if t[constants.TDATE] == '2020/04/02' and t[constants.TYPE] == 'Purchase':\n",
    "#         # sector = dir_utils.search_mapping(sector_df, t[constants.TICKER], sector=True)\n",
    "#         # if sector == 'Financial Services':\n",
    "#         i += 1\n",
    "#         s.add(t_to_obj(t).get_label())\n",
    "#             # print()\n",
    "#             # s.add(t['ticker'])\n",
    "\n",
    "# print(s)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Committees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_committee(group, normalized=None):\n",
    "    # {party_and_gender : 45_ppl}\n",
    "    d = {}\n",
    "        \n",
    "    for _, (_, obj) in group.items(): \n",
    "        for a in obj.get_asgts():\n",
    "            a = a[ : a.find(\" (\")]\n",
    "            d = dict_utils.increment_dictionary(d, a)\n",
    "\n",
    "    # Normalize\n",
    "    if normalized: \n",
    "        d = dict_utils.normalize(d, normalized, percent=True)\n",
    "\n",
    "    return d\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/committees\")\n",
    "\n",
    "d1 = profile_committee(questionable_people_objects)\n",
    "csv_utils.make_csv(dir, \"profile_committee\", d1, [\"committee\", constants.INPUT])\n",
    "\n",
    "d3 = profile_committee(questionable_people_objects, profile_committee(input_all_officials_objects))\n",
    "csv_utils.make_csv(dir, \"profile_committee_normalized\", d3, [\"committee\", constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party and Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_party_and_gender(group, normalized=None):\n",
    "    # {party_and_gender : 45_ppl}\n",
    "    d = {}\n",
    "        \n",
    "    for link, (name, obj) in group.items(): \n",
    "        grouped = obj.party + \", \" + official.get_gender(name, link=link)\n",
    "        d = dict_utils.increment_dictionary(d, grouped)\n",
    "\n",
    "    # Normalize\n",
    "    if normalized: \n",
    "        d = dict_utils.normalize(d, normalized, percent=True)\n",
    "\n",
    "    return d\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/partyandgender\")\n",
    "\n",
    "d1 = profile_party_and_gender(questionable_people_objects)\n",
    "csv_utils.make_csv(dir, \"profile_party_and_gender\", d1, [constants.PARTY + \", \" + constants.GENDER, constants.INPUT])\n",
    "\n",
    "d3 = profile_party_and_gender(questionable_people_objects, profile_party_and_gender(input_all_officials_objects))\n",
    "csv_utils.make_csv(dir, \"profile_party_and_gender_normalized\", d3, [constants.PARTY + \", \" + constants.GENDER, constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age (Lowest, Highest, Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_age(group):\n",
    "    # {age, age, ...}\n",
    "    l = []\n",
    "    \n",
    "    # group = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for _, (_, off_obj) in group.items(): \n",
    "        l.append(off_obj.get_age())\n",
    "    \n",
    "    l.sort()\n",
    "    \n",
    "    d = {}\n",
    "    d[\"0. Youngest\"] = l[0]\n",
    "    d[\"1. Average\"] = round((sum(l) / len(l)))\n",
    "    d[\"2. Oldest\"] = l[len(l)-1]\n",
    "    \n",
    "    return d \n",
    "    \n",
    "d1 = profile_age(questionable_people_objects)\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/age\")\n",
    "csv_utils.make_csv(dir, \"profile_age\", d1, [\"Age\", constants.INPUT])\n",
    "\n",
    "\n",
    "def profile_age_2(group, normalized=None):\n",
    "    # {age : #_of_people, ...}\n",
    "    d = dict(constants.age_groups)\n",
    "    \n",
    "    for (_, off_obj) in group.values(): \n",
    "        age = off_obj.get_age()\n",
    "        d = dict_utils.increment_dictionary(d, ptr_utils.which_age_group(age)) \n",
    "    \n",
    "    if normalized: \n",
    "        d_prime = dict_utils.normalize(d, normalized, percent=True)\n",
    "        return d, d_prime\n",
    "        \n",
    "    return d \n",
    "\n",
    "d1, d4 = profile_age_2(questionable_people_objects, profile_age_2(input_all_officials_objects))\n",
    "\n",
    "csv_utils.make_csv(dir, \"profile_age_2\", (d1), [\"Age\", constants.INPUT])\n",
    "csv_utils.make_csv(dir, \"profile_age_2_normalized\", (d4), [\"Age\", constants.INPUT])\n",
    "\n",
    "\n",
    "def profile_age_3(group):\n",
    "    # {age: freq_count , .... }\n",
    "    d = {}\n",
    "    \n",
    "    # group = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for _, (_, off_obj) in group.items(): \n",
    "        d = dict_utils.increment_dictionary(d, off_obj.get_age())\n",
    "    \n",
    "    return d \n",
    "    \n",
    "d3 = profile_age_3(questionable_people_objects)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/age\")\n",
    "csv_utils.make_csv(dir, \"profile_age_3\", (d3), [\"Age\", constants.INPUT])\n",
    "\n",
    "\n",
    "def profile_age_box(group):\n",
    "    # {age, age, ...}\n",
    "    l = []\n",
    "    \n",
    "    # group = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for _, (_, off_obj) in group.items(): \n",
    "        l.append([off_obj.get_age()])\n",
    "        \n",
    "    return l \n",
    "    \n",
    "d1 = profile_age_box(questionable_people_objects)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/age\")\n",
    "csv_utils.make_csv_base(dir, \"profile_age_box_input\", [\"Age_Input\"], d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oldest and Most Recent Dates (transaction and disclosure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dates(group, type):\n",
    "    lowest_tdate = lowest_ddate = highest_tdate = highest_ddate = None\n",
    "    lowest_tdate_obj = lowest_ddate_obj = highest_tdate_obj = highest_ddate_obj = None\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        curr = t[constants.TDATE]            \n",
    "        if not lowest_tdate or curr < lowest_tdate:\n",
    "            lowest_tdate = curr \n",
    "            lowest_tdate_obj = t\n",
    "        if not highest_tdate or curr > highest_tdate:\n",
    "            highest_tdate = curr  \n",
    "            highest_tdate_obj = t\n",
    "\n",
    "        curr = t[constants.DDATE]\n",
    "        if not lowest_ddate or curr < lowest_ddate:\n",
    "            lowest_ddate = curr \n",
    "            lowest_ddate_obj = t\n",
    "        if not highest_ddate or curr > highest_ddate:\n",
    "            highest_ddate = curr \n",
    "            highest_ddate_obj = t\n",
    "\n",
    "    print(\"Oldest transaction_date for {}: {} by {} \\n {}\".format(type, lowest_tdate, official.get_name(lowest_tdate_obj), lowest_tdate_obj[constants.PTR_LINK]))\n",
    "    print(\"Most recent transaction_date for {}: {} by {} \\n {} \".format(type, highest_tdate, official.get_name(highest_tdate_obj), highest_tdate_obj[constants.PTR_LINK]))\n",
    "    \n",
    "    print(\"Oldest disclosure_date for {}: {} by {} \\n {}\".format(type, lowest_ddate,  official.get_name(lowest_ddate_obj), lowest_ddate_obj[constants.PTR_LINK]))\n",
    "    print(\"Most recent disclosure_date for {}: {} by {}  \\n {}\\n\".format(type, highest_ddate,  official.get_name(highest_ddate_obj), highest_ddate_obj[constants.PTR_LINK]))\n",
    "    \n",
    "\n",
    "profile_dates(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_gender(group, normalized=None):\n",
    "    # d_prime = {'Female' : set(Officials), 'Male' : set(Officials), ...}\n",
    "    d_prime = {}\n",
    "\n",
    "    for link, (name, obj) in group.items(): \n",
    "        d_prime = dict_utils.increment_set_in_dictionary(d_prime, official.get_gender(name, link), name)\n",
    "\n",
    "    # d = {'Female' : #_of_officials, 'Male' : #_of_officials, ...}\n",
    "    d = dict_utils.flatten_len(d_prime, inner_set=True)\n",
    "    \n",
    "    if normalized:\n",
    "        d = dict_utils.normalize(d, normalized, percent=True)\n",
    "        d = dict_utils.sort_dictionary_by_tuple(d)\n",
    "                 \n",
    "    return d\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/gender\")\n",
    "\n",
    "d3 = profile_gender(questionable_people_objects)\n",
    "csv_utils.make_csv(dir, \"profile_gender\", d3, [constants.GENDER, constants.INPUT])\n",
    "\n",
    "d3 = profile_gender(questionable_people_objects, profile_gender(input_all_officials_objects))\n",
    "csv_utils.make_csv(dir, \"profile_gender_normalized\", d3, [constants.GENDER, constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_party(group, normalized=None):\n",
    "    # d_prime = {'Republican' : set(Officials), 'Democrat' : set(Officials), ...}\n",
    "    d_prime = {}\n",
    "    \n",
    "    for (_, off_obj) in group.values(): \n",
    "        d_prime = dict_utils.increment_set_in_dictionary(d_prime, off_obj.party, off_obj.name)\n",
    "    \n",
    "    # d = {'Republican' : #_of_officials, 'Democrat' : #_of_officials, ...}\n",
    "    d = dict_utils.flatten_len(d_prime, inner_set=True)\n",
    "    \n",
    "    if normalized:         \n",
    "        d = dict_utils.normalize(d, normalized, percent=True)\n",
    "        d = dict_utils.sort_dictionary_by_tuple(d)\n",
    "\n",
    "    return d\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/party\")\n",
    "\n",
    "d1 = profile_party(questionable_people_objects, profile_party(input_all_officials_objects))\n",
    "csv_utils.make_csv(dir, \"profile_party_normalized\", (d1), [constants.PARTY, constants.INPUT])\n",
    "\n",
    "d1 = profile_party(questionable_people_objects)\n",
    "csv_utils.make_csv(dir, \"profile_party\", (d1), [constants.PARTY, constants.INPUT])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_state(group, normalized=None):\n",
    "    # d_prime = {'Maryland' : set(Officials), 'California' : set(Officials), ...}\n",
    "    d_prime = {}\n",
    "    \n",
    "    # input_officials_objects = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for _, off_obj in group.values(): \n",
    "        d_prime = dict_utils.increment_set_in_dictionary(d_prime, off_obj.state, off_obj.name)\n",
    "\n",
    "    # d = {'Maryland' : #_of_officials, 'California' : #_of_officials, ...}\n",
    "    d = dict_utils.flatten_len(d_prime, inner_set=True)\n",
    "    \n",
    "    if normalized:\n",
    "        d = dict_utils.normalize(d, normalized, percent=True)\n",
    "        d = dict_utils.sort_dictionary_by_tuple(d)\n",
    "            \n",
    "    return d \n",
    "\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/state\")\n",
    "\n",
    "d1 = profile_state(questionable_people_objects, profile_state(input_all_officials_objects))\n",
    "csv_utils.make_csv(dir, \"profile_state_normalized\", (d1), [constants.STATE, constants.INPUT])\n",
    "\n",
    "d1 = profile_state(questionable_people_objects)\n",
    "csv_utils.make_csv(dir, \"profile_state\", (d1), [constants.STATE, constants.INPUT])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seniority (Lowest, Highest, Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_seniority(group):\n",
    "    # d = {x_years_in_congress, y_years_in_congres, ...}\n",
    "    l = []\n",
    "    \n",
    "    # input_officials_objects = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for _, (_, off_obj) in group.items(): \n",
    "        l.append(off_obj.get_seniority())\n",
    "    \n",
    "    l.sort()\n",
    "    \n",
    "    d = {}\n",
    "    d[\"0. Lowest\"] = l[0]\n",
    "    d[\"1. Average\"] = round((sum(l) / len(l)))\n",
    "    d[\"2. Highest\"] = l[len(l)-1]\n",
    "\n",
    "    return d \n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/seniority\")\n",
    "\n",
    "\n",
    "d3 = profile_seniority(questionable_people_objects)\n",
    "csv_utils.make_csv(dir, \"profile_seniority\", (d3), [constants.SENIORITY, constants.INPUT])\n",
    "\n",
    "\n",
    "def profile_seniority_normalized(group, normalized=None):\n",
    "    d = {}\n",
    "    \n",
    "    for (_, off_obj) in group.values(): \n",
    "        d = dict_utils.increment_dictionary(d, off_obj.get_seniority()) \n",
    "    \n",
    "    if normalized: \n",
    "        d_prime = dict_utils.normalize(d, normalized, percent=True)\n",
    "        return d, d_prime\n",
    "        \n",
    "    return d \n",
    "    \n",
    "d3, d6 = profile_seniority_normalized(questionable_people_objects, profile_seniority_normalized(input_all_officials_objects))\n",
    "csv_utils.make_csv(dir, \"profile_seniority\", (d3), [constants.SENIORITY, constants.INPUT])\n",
    "csv_utils.make_csv(dir, \"profile_seniority_normalized\", (d6), [constants.SENIORITY, constants.INPUT])\n",
    "\n",
    "\n",
    "def profile_seniority_box(group):\n",
    "    # d = {x_years_in_congress, y_years_in_congres, ...}\n",
    "    l = []\n",
    "    \n",
    "    # input_officials_objects = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for _, (_, off_obj) in group.items(): \n",
    "        l.append([off_obj.get_seniority()])\n",
    "    \n",
    "    return l\n",
    "    \n",
    "d1 = profile_seniority_box(questionable_people_objects)\n",
    "csv_utils.make_csv_base(dir, \"profile_seniority_box_input\", [\"Seniority_Input\"], d1)\n",
    "\n",
    "\n",
    "def profile_seniority_2(group):\n",
    "    # d = {x_years_in_congress : #_of_people, }\n",
    "    d = {}\n",
    "    \n",
    "    # input_officials_objects = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for (_, off_obj) in group.values(): \n",
    "        d =  dict_utils.increment_dictionary(d, off_obj.get_seniority())\n",
    "    \n",
    "    return d \n",
    "\n",
    "d3 = profile_seniority_2(questionable_people_objects)\n",
    "\n",
    "\n",
    "csv_utils.make_csv(dir, \"profile_seniority_2\", (d3), [constants.SENIORITY, constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get_Congress (Lowest, Highest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_congress(group):\n",
    "    lowest = highest = None \n",
    "    lowest_person = highest_person = None \n",
    "    \n",
    "    # input_officials_objects = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for (name, off_obj) in group.values():\n",
    "        res = off_obj.get_congress()\n",
    "        \n",
    "        if not lowest or res[0] < lowest:\n",
    "            lowest = res[0]\n",
    "            lowest_person = name\n",
    "    \n",
    "        if not highest or res[len(res) - 1] > highest:\n",
    "            highest = res[len(res) - 1]\n",
    "            highest_person = name \n",
    "                    \n",
    "    d = {}\n",
    "    d[\"Lowest Congress\"] = lowest, lowest_person\n",
    "    d[\"Highest Congress\"] = highest, highest_person\n",
    "\n",
    "    return d \n",
    "                        \n",
    "d3 = profile_congress(questionable_people_objects)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/congress\")\n",
    "csv_utils.make_csv(dir, \"profile_congress\", (d3), [\"\", constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Degrees (Lowest, Highest, Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def profile_degrees_2(group, normalized=None):\n",
    "#     d = {}\n",
    "#     for (_, off_obj) in group.values(): \n",
    "#         d = dict_utils.increment_dictionary(d, off_obj.get_num_of_degrees()) \n",
    "    \n",
    "    \n",
    "#     if normalized: \n",
    "#         d_prime = dict_utils.normalize(d, normalized, percent=True)\n",
    "#         return d, d_prime \n",
    "    \n",
    "#     return d\n",
    "    \n",
    "# d1, d4 = profile_degrees_2(input_house_officials_objects, profile_degrees_2(house_officials_objects))\n",
    "  \n",
    "def profile_degrees(group):    \n",
    "    # d = {x_degrees, y_degrees, ...}\n",
    "    l = []\n",
    "    \n",
    "    # input_officials_objects = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for (_, off_obj) in group.values(): \n",
    "        l.append(off_obj.get_num_of_degrees())\n",
    "    \n",
    "    l.sort()\n",
    "    \n",
    "    d = {}\n",
    "    d[\"0. Lowest\"] = l[0]\n",
    "    d[\"1. Average\"] = round((sum(l) / len(l)))\n",
    "    d[\"2. Highest\"] = l[len(l)-1]\n",
    "\n",
    "    return d \n",
    "    \n",
    "d1 = profile_degrees(questionable_people_objects)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/degrees\")\n",
    "csv_utils.make_csv(dir, \"profile_degrees\", (d1), [\"No. of Degrees\", constants.INPUT]) \n",
    "\n",
    "def profile_degrees_box(group):    \n",
    "    # d = {x_degrees, y_degrees, ...}\n",
    "    l = []\n",
    "    \n",
    "    # input_officials_objects = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for (_, off_obj) in group.values(): \n",
    "        l.append([off_obj.get_num_of_degrees()])\n",
    "    \n",
    "    return l\n",
    "    \n",
    "d2 = profile_degrees_box(questionable_people_objects)\n",
    "csv_utils.make_csv_base(dir, \"profile_degrees_box_input\", [\"Degrees_input\"], d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_JD(group):\n",
    "    # d = {x_degrees, y_degrees, ...}\n",
    "    yes = total = 0 \n",
    "    \n",
    "    # input_officials_objects = {link : (canonical_name_input_based, official_object) ... }\n",
    "    for (_, off_obj) in group.values(): \n",
    "        if off_obj.has_JD():\n",
    "            yes += 1 \n",
    "        total += 1 \n",
    "        \n",
    "    d = {}\n",
    "    \n",
    "    d[\"(Raw, Percent)\"] = (yes, round(yes/total, 2))\n",
    "    \n",
    "    return d \n",
    "    \n",
    "d3 = profile_JD(questionable_people_objects)        \n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/profile/degrees\")\n",
    "csv_utils.make_csv(dir, \"profile_JDs\", (d3), [\"\", constants.INPUT]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Date (transaction_date) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency of Differences between Transaction and Disclosure Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_differences(group):\n",
    "    d = {}\n",
    "    total = num = 0 \n",
    "\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        # Negative, X days BEFORE\n",
    "        # Positive, Y dayas AFTER\n",
    "        diff = ptr_utils.difference_between_dates(t)      \n",
    "        total += 1 \n",
    "        num += diff   \n",
    "            \n",
    "        d =  dict_utils.increment_dictionary(d, int(diff))\n",
    "    \n",
    "    d[\"Average\"] = round(num/total, 2)\n",
    "\n",
    "    return d \n",
    "    # return dict_utils.sort_dictionary_by_keys(d)\n",
    "\n",
    "d3 = frequency_of_differences(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "csv_utils.make_csv(dir, \"frequency_of_differences\", (d3), [\"Difference\", constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_sector(group, diff):\n",
    "    # d_prime = {'sector' : {'date' : #_of_transactions, ....} , 'sector2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            sector = dir_utils.search_mapping(sector_df, t[constants.TICKER], sector=True)            \n",
    "            d_prime =  dict_utils.increment_dictionary_in_dictionary(d_prime, sector, (t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"transaction_date_wrt_sector\"  \n",
    "    if diff:\n",
    "        filename += \"_\" + diff \n",
    "    key_header = constants.SECTOR\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_sector(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each sector controlling for each official. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_sector_controlled(group, diff):    \n",
    "    # d_prime = { ('ticker' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            sector = dir_utils.search_mapping(sector_df, t[constants.TICKER], sector=True)  \n",
    "\n",
    "            d_prime = dict_utils.increment_set_in_inner_dictionary(d_prime, sector, t[constants.TDATE], official.get_name(t))\n",
    "       \n",
    "    \n",
    "    d_prime = dict_utils.flatten_len(d_prime)\n",
    "\n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    \n",
    "    filename = \"transaction_date_wrt_sector_controlled\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff \n",
    "     \n",
    "    key_header = constants.SECTOR\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_sector_controlled(input_df, constants.INPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_industry(group, diff):    \n",
    "    # d_prime = {'industry' : {'date' : #_of_transactions, ....} , 'industry2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(industry_df, t[constants.TICKER], industry=True)\n",
    "\n",
    "            d_prime =  dict_utils.increment_dictionary_in_dictionary(d_prime, industry, (t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"transaction_date_wrt_industry\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff  \n",
    "    key_header = constants.INDUSTRY\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_industry(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each industry controlling for official. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_industry_controlled(group, diff):\n",
    "    \n",
    "    # d_prime = { ('ticker' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(industry_df, t[constants.TICKER], industry=True)\n",
    "            \n",
    "            d_prime = dict_utils.increment_set_in_inner_dictionary(d_prime, industry, t[constants.TDATE],  official.get_name(t))\n",
    "\n",
    "    d_prime = dict_utils.flatten_len(d_prime)\n",
    "    \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "    \n",
    "    filename = \"transaction_date_wrt_industry_controlled\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff  \n",
    "    key_header = constants.INDUSTRY\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_industry_controlled(input_df, constants.INPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each ticker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_ticker(group, diff):    \n",
    "    # d_prime = {'ticker' : {'date' : #_of_transactions, ....} , 'ticker2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d_prime =  dict_utils.increment_dictionary_in_dictionary(d_prime, t[constants.TICKER], (t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"transaction_date_wrt_ticker\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff\n",
    "    key_header = constants.TICKER\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_ticker(input_df, constants.INPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each ticker controlling for official."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_ticker_controlled(group, diff):    \n",
    "    # d_prime = { ('ticker' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():    \n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):    \n",
    "            name = official.get_name(t)\n",
    "            d_prime = dict_utils.increment_set_in_inner_dictionary(d_prime, t[constants.TICKER], t[constants.TDATE], name)\n",
    "       \n",
    "    d_prime = dict_utils.flatten_len_inner_set(d_prime)\n",
    "\n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    \n",
    "    filename = \"transaction_date_wrt_ticker_controlled\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff  \n",
    "        \n",
    "    key_header = constants.TICKER\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_ticker_controlled(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_type(group, diff):        \n",
    "    # d_prime = {'type' : {'date' : #_of_transactions, ....} , 'type2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d_prime =  dict_utils.increment_dictionary_in_dictionary(d_prime, ptr_utils.format_type(t[constants.TYPE]), (t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'type' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"transaction_date_wrt_type\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff  \n",
    "    key_header = constants.TYPE\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df)\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_type(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for type controlling for official. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_type_controlled(group, diff):    \n",
    "    # d_prime = { ('type' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d_prime = dict_utils.increment_set_in_inner_dictionary(d_prime, t[constants.TYPE], (t[constants.TDATE]), official.get_name(t))\n",
    "       \n",
    "\n",
    "    d_prime = dict_utils.flatten_len(d_prime)\n",
    "            \n",
    "    # d = {'type' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "    \n",
    "    \n",
    "    filename = \"transaction_date_wrt_type_controlled\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff  \n",
    "    key_header = constants.TYPE\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_type_controlled(input_df, constants.INPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_amount(group, diff):\n",
    "    # d_prime = {'amount' : {'date' : #_of_transactions, ....} , 'amount1' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            amount = ptr_utils.consistency_amount(t)\n",
    "            d_prime =  dict_utils.increment_dictionary_in_dictionary(d_prime, amount, (t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'type' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.add_sort_key_for_amount(d)\n",
    "    d = dict_utils.sort_dictionary_by_sort_key(d)\n",
    "\n",
    "    filename = \"transaction_date_wrt_amount\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff  \n",
    "    key_header = constants.TYPE\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, key_header)\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_amount(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for amount controlling for official."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_amount_controlled(group, diff):\n",
    "    # d_prime = { ('amount' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.AMOUNT]):\n",
    "            amount = ptr_utils.consistency_amount(t)\n",
    "            d_prime = dict_utils.increment_set_in_inner_dictionary(d_prime, amount, (t[constants.TDATE]), official.get_name(t))\n",
    "       \n",
    "\n",
    "    d_prime = dict_utils.flatten_len(d_prime)\n",
    "        \n",
    "    # d = {'amount' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "    d = dict_utils.add_sort_key_for_amount(d)\n",
    "    d = dict_utils.sort_dictionary_by_sort_key(d)\n",
    "    \n",
    "    \n",
    "    filename = \"transaction_date_wrt_amount_controlled\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff  \n",
    "    key_header = constants.AMOUNT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, key_header)\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "_ = transaction_date_wrt_amount_controlled(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each official."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_date_wrt_official(group, diff):\n",
    "    # d_prime = {'person1' : {'date' : #_of_transactions, ....} , 'person2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d_prime =  dict_utils.increment_dictionary_in_dictionary(d_prime, official.get_name(t), (t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'person' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten_best(d_prime)\n",
    "\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"transaction_date_wrt_official\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff \n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [constants.OFFICIAL, constants.TDATE, constants.NUMT])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_official_res = transaction_date_wrt_official(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Transactions per Date Controlled\n",
    "_Number of transactions per date controlled by official. E.g. if Ted Baker made 40 transactions on 1/1/02 and Sam Wall made 2 transactions on 1/1/02, we conclude that there were two transactions on 1/1/02._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_per_date_controlled(group):    \n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        d =  dict_utils.increment_dictionary_in_dictionary(d, (t[constants.TDATE]), official.get_name(t))\n",
    "\n",
    "    return dict_utils.flatten_len(d, inner_set=True)\n",
    "    \n",
    "d3 = num_of_trans_per_date_controlled(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "csv_utils.make_csv(dir, \"num_of_trans_per_date_controlled\", (d3), [constants.TDATE, constants.INPUT])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Transactions Within 2 Weeks Prior to Quarterly Tax Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_within_tax_date(group):        \n",
    "        total = within = 0 \n",
    "        d = {}\n",
    "\n",
    "        for _,t in group.iterrows():  \n",
    "                total += 1 \n",
    "                if ptr_utils.within_tax_date(t[constants.TDATE]):\n",
    "                        within += 1 \n",
    "\n",
    "        d[\"(No. of transactions within 2 weeks of tax deadline, %)\"] = (within,  round(within/total, 2))\n",
    "\n",
    "        return d \n",
    "\n",
    "d3 = num_of_trans_within_tax_date(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "csv_utils.make_csv(dir, \"num_of_trans_within_tax_date\", (d3), [\"\",  constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Transactions Within 2 Weeks Prior to Quarterly Tax Date Semi-Controlled \n",
    "\n",
    "_Given dict='09/03/2021': {'Thomas H Tuberville': 1, 'Cynthia M Lummis': 1, 'A. Mitchell Mcconnell, Jr.': 1}...I only incremement the number of within (tax date) once per date per official. So, if an official does 100 transactions on a date within two weeks of a quarterly deadline, then I only count it as one transaction._\n",
    "\n",
    "_A Note: total === number of transactions per person per date (so not really all transactions) because someone could have potentially made 60 transactions on one date which we don't include in neither total or within, if applicable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_within_tax_date_controlled(group):\n",
    "        total = within = 0 \n",
    "        people = set()\n",
    "        d = {}\n",
    "\n",
    "        for _, t in group.iterrows():\n",
    "                name = official.get_name(t)\n",
    "                if ptr_utils.within_tax_date(t[constants.TDATE]) and name not in people:\n",
    "                        people.add(name)\n",
    "                        within += 1 \n",
    "                total += 1         \n",
    "                \n",
    "        d[\"(No. of transactions within 2 weeks of tax deadline, %)\"] = (within,  round(within/total, 2))\n",
    "\n",
    "        return d\n",
    "\n",
    "d3 = num_of_trans_within_tax_date_controlled(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TDATE)\n",
    "csv_utils.make_csv(dir, \"num_of_trans_within_tax_date_controlled\", (d3), [\"\", constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owner (owner) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Count of Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_count_of_owner(group):\n",
    "    # d = {'Joint' : 5}\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        if constants.OWNER in t and ptr_utils.isvalid(t[constants.OWNER]):                \n",
    "            d =  dict_utils.increment_dictionary(d, t[constants.OWNER].capitalize())\n",
    "        \n",
    "    return dict_utils.sort_dictionary_by_values(d)\n",
    "\n",
    "# {link : (canonical_name_input_based, official_object) ... }\n",
    "d3 = freq_count_of_owner(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.OWNER)\n",
    "csv_utils.make_csv(dir, \"freq_count_of_owner\", (d3), [\"Owner\", constants.INPUT]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_count_by_spouse(group):\n",
    "    # d = {'x_spouse' : 5}\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        if constants.OWNER in t and ptr_utils.isvalid(t[constants.OWNER]) and t[constants.OWNER].capitalize() == 'Spouse':\n",
    "            d =  dict_utils.increment_dictionary(d, official.get_name(t))\n",
    "        \n",
    "    return d\n",
    "    \n",
    "    \n",
    "# {link : (canonical_name_input_based, official_object) ... }\n",
    "d3 = freq_count_by_spouse(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.OWNER)\n",
    "csv_utils.make_csv(dir, \"freq_count_by_spouse\", (d3), [\"Owner\", constants.INPUT]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticker (ticker) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_tickers(group):\n",
    "    # d = {'ticker' : #_of_times }\n",
    "    d = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d =  dict_utils.increment_dictionary(d, t[constants.TICKER])\n",
    "       \n",
    "\n",
    "    return dict_utils.sort_dictionary_by_values(d, reverse=True)\n",
    "\n",
    "d3 = num_of_tickers(input_df)\n",
    "    \n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TICKER)\n",
    "csv_utils.make_csv(dir, \"num_of_tickers\", (d3), [constants.TICKER,  constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Ticker per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_ticker_breakdown_year(group, diff):\n",
    "    d = {}\n",
    "\n",
    "    # {\"ticker\" : {\"year\" : number, \"year\" : number, ...}}\n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d = dict_utils.increment_dictionary_in_dictionary(d, t[constants.TICKER], ptr_utils.get_year(t[constants.TDATE]))\n",
    "            d = dict_utils.increment_dictionary_in_dictionary(d, t[constants.TICKER], 9999)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_keys(d)\n",
    "    filename = \"frequency_of_ticker_breakdown_year\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff\n",
    "    key_header = constants.TICKER\n",
    "    \n",
    "    \n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TICKER)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "\n",
    "frequency_of_ticker_breakdown_year(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Ticker per Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_ticker_by_date(group, diff):\n",
    "    # {ticker : {date : ___}}\n",
    "    d = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d = dict_utils.increment_dictionary_in_dictionary(d, t[constants.TICKER], t[constants.TDATE])\n",
    "\n",
    "    \n",
    "    d = dict_utils.flatten_best(d)\n",
    "    \n",
    "\n",
    "    filename = \"frequency_of_ticker_by_date\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff\n",
    "    \n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TICKER)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, constants.TICKER)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "\n",
    "d3 = frequency_of_ticker_by_date(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry & Sector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Transactions per Industry\n",
    "_Not controlled in any way._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_transactions_per_indusry(group):        \n",
    "    d = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(industry_df, t[constants.TICKER], industry=True)\n",
    "            d = dict_utils.increment_dictionary(d, industry)\n",
    "\n",
    "    return dict_utils.sort_dictionary_by_values(d)\n",
    "    \n",
    "\n",
    "d3 = number_of_transactions_per_indusry(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.INDUSTRY)\n",
    "csv_utils.make_csv(dir, \"number_of_transactions_per_indusry\", (d3), [constants.INDUSTRY, constants.INPUT])\n",
    "\n",
    "\n",
    "\n",
    "def number_of_transactions_per_sector(group):        \n",
    "    d = {}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            sector = dir_utils.search_mapping(sector_df, t[constants.TICKER], sector=True)\n",
    "            d = dict_utils.increment_dictionary(d, sector)\n",
    "\n",
    "    return dict_utils.sort_dictionary_by_values(d)\n",
    "    \n",
    "\n",
    "d3 = number_of_transactions_per_sector(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.SECTOR)\n",
    "csv_utils.make_csv(dir, \"number_of_transactions_per_sector\", (d3), [constants.SECTOR, constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industry Breakdown per Official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_industry_breakdown_official(group, diff):\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(industry_df, t[constants.TICKER], industry=True)\n",
    "            d = dict_utils.increment_dictionary_in_dictionary(d, official.get_name(t), industry)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"frequency_of_industry_breakdown_official\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.INDUSTRY)\n",
    "\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, constants.OFFICIAL)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "    \n",
    "    return d \n",
    "\n",
    "d3 = frequency_of_industry_breakdown_official(input_df, constants.INPUT)\n",
    "\n",
    "\n",
    "\n",
    "def frequency_of_sector_breakdown_official(group, diff):\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            sector = dir_utils.search_mapping(sector_df, t[constants.TICKER], sector=True)\n",
    "            d = dict_utils.increment_dictionary_in_dictionary(d, official.get_name(t), sector)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"frequency_of_sector_breakdown_official\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.SECTOR)\n",
    "\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, constants.OFFICIAL)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "    \n",
    "    return d \n",
    "\n",
    "d3 = frequency_of_sector_breakdown_official(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency of Industry per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_industry_breakdown(group, diff):\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():     \n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(industry_df, t[constants.TICKER], industry=True)  \n",
    "            d = dict_utils.increment_dictionary_in_dictionary(d, industry, ptr_utils.get_year(t[constants.TDATE]))\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"frequency_of_industry_breakdown\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff \n",
    "    key_header = \"industry\"\n",
    "    \n",
    "    \n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.INDUSTRY)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "    return d \n",
    "\n",
    "d3 = frequency_of_industry_breakdown(input_df, constants.INPUT)\n",
    "\n",
    "\n",
    "def frequency_of_sector_breakdown(group, diff):\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():     \n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            sector = dir_utils.search_mapping(sector_df, t[constants.TICKER], sector=True)  \n",
    "            d = dict_utils.increment_dictionary_in_dictionary(d, sector, ptr_utils.get_year(t[constants.TDATE]))\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"frequency_of_sector_breakdown\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff \n",
    "    key_header = \"sector\"\n",
    "    \n",
    "    \n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.SECTOR)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "    return d \n",
    "\n",
    "d3 = frequency_of_sector_breakdown(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asset Description (asset_description) and Comment (comment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_options(group):\n",
    "    count = total = 0 \n",
    "    d = {}\n",
    "    # [this_person_placed_an_option, ...]\n",
    "    people = set()\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        if constants.ASSET_DESCRIPTION in t and ptr_utils.isvalid(t[constants.ASSET_DESCRIPTION]) and (\"Put\" in t[constants.ASSET_DESCRIPTION]  or \"put\" in t[constants.ASSET_DESCRIPTION]  or \"Call\" in t[constants.ASSET_DESCRIPTION]  or \"call\" in t[constants.ASSET_DESCRIPTION]  or \"Option\" in t[constants.ASSET_DESCRIPTION] or \"option\" in t[constants.ASSET_DESCRIPTION]): \n",
    "            count += 1 \n",
    "            people.add(official.get_name(t))\n",
    "        total += 1 \n",
    "\n",
    "    d[\"(No. of Options, %)\"] = (count, round(count/total, 2))\n",
    "\n",
    "    return d \n",
    "\n",
    "d3 = number_of_options(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.COMMENT)\n",
    "csv_utils.make_csv(dir, \"number_of_options\", (d3), [\"\",  constants.INPUT])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Scanned PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_scanned_pdfs(group):\n",
    "    count = total = 0 \n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        if t[constants.ASSET_DESCRIPTION] == constants.DISCLOSED:\n",
    "            count += 1 \n",
    "        total += 1 \n",
    "            \n",
    "    d[\"(No. of Scanned PDFS, %)\"] = (count, round(count/total, 2))\n",
    "\n",
    "    return d \n",
    "            \n",
    "d3 = number_of_scanned_pdfs(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.COMMENT)\n",
    "csv_utils.make_csv(dir, \"number_of_scanned_pdfs\", (d3), [\"\", constants.INPUT])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asset Type (asset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Asset Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_asset_type(group):\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        if constants.ATYPE in t and ptr_utils.isvalid(t[constants.ATYPE]):\n",
    "            d = dict_utils.increment_dictionary(d, t[constants.ATYPE])\n",
    "      \n",
    "    return dict_utils.sort_dictionary_by_values(d)\n",
    "\n",
    "d3 = frequency_of_asset_type(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.ATYPE)\n",
    "csv_utils.make_csv(dir, \"frequency_of_asset_type\", (d3), [constants.ATYPE, constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Amount by Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_amount_by_persom(group, diff):\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        amount = ptr_utils.consistency_amount(t)\n",
    "        d = dict_utils.increment_dictionary_in_dictionary(d, official.get_name(t), amount)\n",
    "    \n",
    "    d = dict_utils.sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"frequency_of_amount_by_person\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff \n",
    "        \n",
    "    key_header = constants.AMOUNT\n",
    "    \n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.AMOUNT)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "    \n",
    "    return d \n",
    "   \n",
    "    \n",
    "d2 = frequency_of_amount_by_persom(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Transactions by Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_amount_total(group):\n",
    "    d = {}        \n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        amount = ptr_utils.consistency_amount(t)                    \n",
    "        d = dict_utils.increment_dictionary(d, amount)\n",
    "\n",
    "    return d\n",
    "    \n",
    "d3 = frequency_of_amount_total(input_df)\n",
    "\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.AMOUNT)\n",
    "csv_utils.make_csv(dir, \"frequency_of_amount_total\", (d3), [constants.AMOUNT, constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Amount by Gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def frequency_of_amount_by_gender(group, diff, normalized=None):\n",
    "#     d = {}\n",
    "    \n",
    "#     for _,t in group.iterrows():\n",
    "#         name = official.get_name(t)\n",
    "#         link = input_all_officials_name[name]\n",
    "#         amount = ptr_utils.consistency_amount(t)\n",
    "#         d = dict_utils.increment_dictionary_in_dictionary(d, amount, all_officials_gender[link])\n",
    "\n",
    "#     if normalized: \n",
    "#         for amount in d:\n",
    "#             for gender in d[amount]:\n",
    "#                 d[amount][gender] /= normalized[gender]\n",
    "        \n",
    "#     d = dict_utils.add_sort_key_for_amount(d)\n",
    "#     d = dict_utils.sort_dictionary_by_sort_key(d)\n",
    "\n",
    "\n",
    "#     filename = \"frequency_of_amount_by_gender\"\n",
    "#     if diff:\n",
    "#         filename += \"_\" + diff\n",
    "#     if normalized:\n",
    "#         filename += \"_normalized\"\n",
    "    \n",
    "#     dir = dir_utils.makesubdir(constants.path_csv, constants.AMOUNT)\n",
    "#     wd = csv_utils.make_csv_breakdown(dir, filename, d, constants.AMOUNT)\n",
    "#     print(pd.read_csv(wd).head(2))\n",
    "#     return d \n",
    "   \n",
    "# d1 = frequency_of_amount_by_gender(house_input_df, constants.HOUSE, profile_gender(input_house_officials_link))\n",
    "# d2 = frequency_of_amount_by_gender(senate_input_df, constants.SENATE, profile_gender(input_senate_officials_link))\n",
    "# d3 = frequency_of_amount_by_gender(input_df, constants.INPUT, profile_gender(input_all_officials_link))\n",
    "    \n",
    "# d1 = frequency_of_amount_by_gender(house_input_df, constants.HOUSE)\n",
    "# d2 = frequency_of_amount_by_gender(senate_input_df, constants.SENATE)\n",
    "# d3 = frequency_of_amount_by_gender(input_df, constants.INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Transactions by Political Affiliation and Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def frequency_of_amount_by_aff(group, diff):\n",
    "#     d = {}\n",
    "\n",
    "#     for _,t in group.iterrows():\n",
    "#         obj = t_to_obj(t)\n",
    "#         amount = ptr_utils.consistency_amount(t)            \n",
    "#         d = dict_utils.increment_dictionary_in_dictionary(d, amount, obj.party)\n",
    "\n",
    "\n",
    "#     d = dict_utils.add_sort_key_for_amount(d)\n",
    "#     d = dict_utils.sort_dictionary_by_sort_key(d)\n",
    "    \n",
    "        \n",
    "#     filename = \"frequency_of_amount_by_aff\"\n",
    "#     if diff:\n",
    "#         filename += \"_\" + diff\n",
    "    \n",
    "#     dir = dir_utils.makesubdir(constants.path_csv, constants.AMOUNT)\n",
    "#     wd = csv_utils.make_csv_breakdown(dir, filename, d, constants.AMOUNT)\n",
    "#     print(pd.read_csv(wd).head(2))\n",
    "    \n",
    "#     return d \n",
    "   \n",
    "    \n",
    "# d1 = frequency_of_amount_by_aff(house_input_df, constants.HOUSE)\n",
    "# d2 = frequency_of_amount_by_aff(senate_input_df, constants.SENATE)\n",
    "# d3 = frequency_of_amount_by_aff(input_df, constants.INPUT) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def average_activity(group, diff):\n",
    "#     d={}\n",
    "    \n",
    "#     for _,t in group.iterrows():\n",
    "#         if ptr_utils.isvalid(t[constants.AMOUNT]):\n",
    "                        \n",
    "#             mean = ptr_utils.get_gmean(t[constants.AMOUNT])\n",
    "\n",
    "#             d = dict_utils.increment_list_in_dictionary(d, official.get_name(t), mean)\n",
    "\n",
    "\n",
    "#     d = dict_utils.flatten_gmean(d)        \n",
    "            \n",
    "#     filename = \"average_activity\"\n",
    "#     if diff:\n",
    "#         filename += \"_\" + diff\n",
    "\n",
    "#     key_header = constants.OFFICIAL \n",
    "#     value_header = \"average_size_of_transactions\"\n",
    "\n",
    "#     d = dict_utils.sort_dictionary_by_values(d)\n",
    "#     d = dict_utils.commify(d)\n",
    "\n",
    "#     dir = dir_utils.makesubdir(constants.path_csv, constants.AMOUNT)\n",
    "#     wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header])\n",
    "#     df = pd.read_csv(wd)\n",
    "#     print(df.head(5))\n",
    "\n",
    "#     return d \n",
    "\n",
    "# d1 = average_activity(house_input_df, constants.HOUSE)\n",
    "# d2 = average_activity(senate_input_df, constants.SENATE)\n",
    "# d3 = average_activity(input_df, constants.INPUT) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types (type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Actions Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_act(group):\n",
    "    d = {}\n",
    "\n",
    "    for _,t in group.iterrows():\n",
    "        if ptr_utils.isvalid(t[constants.TYPE]):\n",
    "            d = dict_utils.increment_dictionary(d, t[constants.TYPE])\n",
    "    \n",
    "    return dict_utils.sort_dictionary_by_values(d)\n",
    "    \n",
    "     \n",
    "d3 = frequency_of_act(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TYPE)\n",
    "csv_utils.make_csv(dir, \"frequency_of_act\", (d3), [constants.TYPE,  constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Transactions per Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def types_of_transactions_per_person(group, diff, normalized=None):\n",
    "    d={}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        d = dict_utils.increment_dictionary_in_dictionary(d, official.get_name(t), t[constants.TYPE])\n",
    "        d = dict_utils.increment_dictionary_in_dictionary(d, official.get_name(t), constants.TOTAL)\n",
    "\n",
    "\n",
    "    if normalized: \n",
    "        for k,v in d.items():\n",
    "            newinner = {}\n",
    "            for ik, iv in v.items():\n",
    "                link = input_all_officials_name[k]\n",
    "                _, obj = input_all_officials_objects[link]\n",
    "                newinner[ik] = round(iv/obj.get_seniority(), 2)\n",
    "                \n",
    "            d[k] = newinner\n",
    "    \n",
    "    filename = \"types_of_transactions_per_person\"\n",
    "    if diff:\n",
    "        filename += \"_\" + diff\n",
    "    if normalized:\n",
    "        filename += \"_normalized\"\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_keys(d)\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.TYPE)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, constants.OFFICIAL)\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "\n",
    "    return d \n",
    "\n",
    "d3 = types_of_transactions_per_person(input_df, constants.INPUT)\n",
    "\n",
    "d3 = types_of_transactions_per_person(input_df, constants.INPUT, normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_per_year(group, normalized=None):\n",
    "    d={}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        d =  dict_utils.increment_dictionary(d, ptr_utils.get_year(t[constants.TDATE]))\n",
    "\n",
    "    if normalized:\n",
    "        d2 = {}\n",
    "        for k,v in d.items():\n",
    "            d2[k] = v/normalized\n",
    "            \n",
    "        \n",
    "    return dict_utils.sort_dictionary_by_values(d), dict_utils.sort_dictionary_by_values(d2)\n",
    "\n",
    "d3, d6 = num_of_trans_per_year(input_df, len(input_all_officials_objects))\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.FREQ)\n",
    "csv_utils.make_csv(dir, \"num_of_trans_per_year\", (d3), [\"year\", constants.INPUT])\n",
    "csv_utils.make_csv(dir, \"num_of_trans_per_year_normalized\", d6, [\"year\", constants.INPUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Person \n",
    "_Not controlling for number of years in position or size of transaction._ For each official, we want their total number of transactions.  {'Sam': 5, 'Alex': 2424, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_per_person(group):\n",
    "    d={}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        d = dict_utils.increment_dictionary(d, official.get_name(t))\n",
    "\n",
    "    return dict_utils.sort_dictionary_by_values(d)\n",
    "    \n",
    "d3 = num_of_trans_per_person(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.PROFILE)\n",
    "csv_utils.make_csv(dir, \"num_of_trans_per_person\", (d3), [constants.OFFICIAL, constants.INPUT])\n",
    "\n",
    "\n",
    "def num_of_trans_per_person_normalized(group):\n",
    "    d={}\n",
    "    \n",
    "    for _,t in group.iterrows():\n",
    "        d = dict_utils.increment_dictionary(d, official.get_name(t))\n",
    "\n",
    "    new_d = {}\n",
    "    for k,v in d.items():\n",
    "        link = input_all_officials_name[k]\n",
    "        _, obj = input_all_officials_objects[link]\n",
    "        new_d[k] = round(v/obj.get_seniority(), 2) if round(v/obj.get_seniority(), 2) != 0 else 1 \n",
    "        \n",
    "    return new_d\n",
    "    \n",
    "\n",
    "d3 = num_of_trans_per_person_normalized(input_df)\n",
    "\n",
    "dir = dir_utils.makesubdir(constants.path_csv, \"insider_trading/\"+constants.PROFILE)\n",
    "csv_utils.make_csv(dir, \"num_of_trans_per_person_normalized\", (d3), [constants.OFFICIAL, constants.INPUT])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
