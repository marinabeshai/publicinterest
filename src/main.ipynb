{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.csv_utils as csv_utils \n",
    "import utils.dir_utils as dir_utils\n",
    "import utils.dict_utils as dict_utils \n",
    "import utils.ptr_utils as ptr_utils\n",
    "import utils.constants as constants \n",
    "import helpers.official_u as official\n",
    "import pandas as pd \n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "title, rows = dir_utils.get_data(senate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Date (transaction_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   sector transaction_date  number_of_transactions\n",
      "0      Financial Services       2020/04/14                      27\n",
      "1                    Fund       2020/04/02                      26\n",
      "2                     NaN       2015/02/13                      19\n",
      "3  Communication Services       2020/04/14                      18\n",
      "4              Technology       2020/04/14                      18\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_sector():\n",
    "    \n",
    "    df = dir_utils.get_mapping(sector=True)\n",
    "\n",
    "    # d_prime = {'industry' : {'date' : #_of_transactions, ....} , 'industry2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(df, t[constants.TICKER], sector=True)\n",
    "            \n",
    "            d_prime = dict_utils.increment_dictionary_in_dictionary(d_prime, industry, ptr_utils.format_date(t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"most_popular_td_fe_sector\"\n",
    "    key_header = constants.SECTOR\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_sector_res = transaction_date_wrt_sector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each sector controlling for each official."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   sector transaction_date  number_of_transactions\n",
      "0      Financial Services       2020/03/09                       4\n",
      "1              Technology       2020/08/27                       3\n",
      "2  Communication Services       2020/08/21                       3\n",
      "3                     NaN       2020/04/14                       3\n",
      "4              Healthcare       2020/04/14                       3\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_sector_controlled():\n",
    "\n",
    "    df = dir_utils.get_mapping(sector=True)\n",
    "\n",
    "    # d_prime = { ('ticker' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(df, t[constants.TICKER], sector=True)\n",
    "            \n",
    "            d_prime = dict_utils.increment_set_in_dictionary(d_prime, industry, ptr_utils.format_date(t[constants.TDATE]), official.get_canonical_name(t[title]))\n",
    "       \n",
    "\n",
    "    for ticker in d_prime:\n",
    "        for date in d_prime[ticker]:\n",
    "            d_prime[ticker][date] = len(d_prime[ticker][date])\n",
    "            \n",
    "    \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "   \n",
    "    \n",
    "    filename = \"most_popular_td_fe_sector_controlled\"\n",
    "    key_header = constants.SECTOR\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_sector_controlled_res = transaction_date_wrt_sector_controlled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     industry transaction_date  number_of_transactions\n",
      "0                        Fund       2020/04/02                      26\n",
      "1                         NaN       2015/02/13                      19\n",
      "2         Oil & Gas Midstream       2020/04/15                      15\n",
      "3  Drug Manufacturers—General       2020/04/14                      10\n",
      "4               Entertainment       2020/04/14                      10\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_industry():\n",
    "    \n",
    "    df = dir_utils.get_mapping()\n",
    "\n",
    "    # d_prime = {'industry' : {'date' : #_of_transactions, ....} , 'industry2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(df, t[constants.TICKER])\n",
    "            \n",
    "            d_prime = dict_utils.increment_dictionary_in_dictionary(d_prime, industry, ptr_utils.format_date(t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"most_popular_td_fe_industry\"\n",
    "    key_header = constants.INDUSTRY\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_industry_res = transaction_date_wrt_industry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each industry controlling for official."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     industry transaction_date  number_of_transactions\n",
      "0        Consumer Electronics       2020/08/27                       3\n",
      "1             Credit Services       2020/04/14                       3\n",
      "2                         NaN       2020/04/14                       3\n",
      "3               Entertainment       2020/04/06                       3\n",
      "4  Drug Manufacturers—General       2016/01/28                       3\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_industry_controlled():\n",
    "    \n",
    "    df = dir_utils.get_mapping()\n",
    "\n",
    "    # d_prime = { ('ticker' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            industry = dir_utils.search_mapping(df, t[constants.TICKER])\n",
    "            \n",
    "            d_prime = dict_utils.increment_set_in_dictionary(d_prime, industry, ptr_utils.format_date(t[constants.TDATE]), official.get_canonical_name(t[title]))\n",
    "       \n",
    "\n",
    "    for ticker in d_prime:\n",
    "        for date in d_prime[ticker]:\n",
    "            d_prime[ticker][date] = len(d_prime[ticker][date])\n",
    "            \n",
    "    \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "    \n",
    "    \n",
    "    filename = \"most_popular_td_fe_industry_controlled\"\n",
    "    key_header = constants.INDUSTRY\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_industry_controlled_res = transaction_date_wrt_industry_controlled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each ticker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker transaction_date  number_of_transactions\n",
      "0   ECOM       2021/02/10                       8\n",
      "1      X       2021/05/06                       6\n",
      "2    OXY       2021/02/16                       6\n",
      "3    CLF       2021/07/21                       5\n",
      "4     AA       2021/01/06                       5\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_ticker():\n",
    "        \n",
    "    # d_prime = {'ticker' : {'date' : #_of_transactions, ....} , 'ticker2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d_prime = dict_utils.increment_dictionary_in_dictionary(d_prime, t[constants.TICKER], ptr_utils.format_date(t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"most_popular_td_fe_ticker\"\n",
    "    key_header = constants.TICKER\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_ticker_res = transaction_date_wrt_ticker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each ticker controlling for official."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker transaction_date  number_of_transactions\n",
      "0     --       2015/12/14                       4\n",
      "1   AAPL       2020/08/27                       3\n",
      "2   DWDP       2017/09/01                       3\n",
      "3    NaN       2016/08/12                       3\n",
      "4   AMZN       2020/06/26                       2\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_ticker_controlled():\n",
    "    # d_prime = { ('ticker' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "\n",
    "    \n",
    "    for _,t in rows:\n",
    "       d_prime = dict_utils.increment_set_in_dictionary(d_prime, t[constants.TICKER], ptr_utils.format_date(t[constants.TDATE]), official.get_canonical_name(t[title]))\n",
    "       \n",
    "\n",
    "    for ticker in d_prime:\n",
    "        for date in d_prime[ticker]:\n",
    "            d_prime[ticker][date] = len(d_prime[ticker][date])\n",
    "            \n",
    "    \n",
    "    # d = {'ticker' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    \n",
    "    filename = \"most_popular_td_fe_ticker_controlled\"\n",
    "    key_header = constants.TICKER\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_ticker_controlled_res = transaction_date_wrt_ticker_controlled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             type transaction_date  number_of_transactions\n",
      "0     Sale (Full)       2020/04/14                     116\n",
      "1        Purchase       2017/03/16                      78\n",
      "2  Sale (Partial)       2020/04/14                      26\n",
      "3        Exchange       2017/09/01                       5\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_type():\n",
    "        \n",
    "    # d_prime = {'type' : {'date' : #_of_transactions, ....} , 'type2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d_prime = dict_utils.increment_dictionary_in_dictionary(d_prime, t[constants.TYPE], ptr_utils.format_date(t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'type' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"most_popuar_td_for_type\"\n",
    "    key_header = constants.TYPE\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_type_res = transaction_date_wrt_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for type controlling for official. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             type transaction_date  number_of_transactions\n",
      "0     Sale (Full)       2020/07/07                       4\n",
      "1        Purchase       2019/02/27                       4\n",
      "2  Sale (Partial)       2020/03/17                       3\n",
      "3        Exchange       2017/09/01                       3\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_type_controlled():\n",
    "    \n",
    "    # d_prime = { ('type' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            # print( t[constants.TYPE], ptr_utils.format_date(t[constants.TDATE]), official.get_canonical_name(t[title]))\n",
    "            d_prime = dict_utils.increment_set_in_dictionary(d_prime, t[constants.TYPE], ptr_utils.format_date(t[constants.TDATE]), official.get_canonical_name(t[title]))\n",
    "       \n",
    "\n",
    "    for ticker in d_prime:\n",
    "        for date in d_prime[ticker]:\n",
    "            d_prime[ticker][date] = len(d_prime[ticker][date])\n",
    "            \n",
    "    \n",
    "    # d = {'type' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "    \n",
    "    \n",
    "    filename = \"most_popular_td_fe_type_controlled\"\n",
    "    key_header = constants.TYPE\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_type_controlled_res = transaction_date_wrt_type_controlled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dir_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y3/llvqw9gd7nz5vkbmv014r2kc0000gn/T/ipykernel_23556/978619170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtransaction_date_wrt_amount_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransaction_date_wrt_amount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/y3/llvqw9gd7nz5vkbmv014r2kc0000gn/T/ipykernel_23556/978619170.py\u001b[0m in \u001b[0;36mtransaction_date_wrt_amount\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransaction_date_wrt_amount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# d_prime = {'amount' : {'date' : #_of_transactions, ....} , 'amount1' : .... }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dir_utils' is not defined"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_amount():\n",
    "    # d_prime = {'amount' : {'date' : #_of_transactions, ....} , 'amount1' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d_prime = dict_utils.increment_dictionary_in_dictionary(d_prime, t[constants.AMOUNT], ptr_utils.format_date(t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'type' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "    d = dict_utils.add_sort_key_for_amount(d)\n",
    "    d = dict_utils.sort_dictionary_by_sort_key(d)\n",
    "\n",
    "    filename = \"most_popuar_td_for_amount\"\n",
    "    key_header = constants.TYPE\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, key_header)\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_amount_res = transaction_date_wrt_amount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for amount controlling for official."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                amount  2015/02/20  2015/10/05  2016/02/11  2016/08/12  \\\n",
      "0              Unknown         NaN         NaN         NaN         3.0   \n",
      "1     $1,001 - $15,000         NaN         NaN         5.0         NaN   \n",
      "2    $15,001 - $50,000         NaN         NaN         NaN         NaN   \n",
      "3   $50,001 - $100,000         NaN         NaN         NaN         NaN   \n",
      "4  $100,001 - $250,000         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   2017/02/01  2018/12/27  2019/10/11  2020/07/31  2021/03/01  2021/03/29  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2         5.0         NaN         NaN         NaN         NaN         NaN   \n",
      "3         NaN         NaN         NaN         3.0         NaN         NaN   \n",
      "4         NaN         3.0         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   2021/07/23  sort_key  \n",
      "0         NaN        -1  \n",
      "1         NaN     15000  \n",
      "2         NaN     50000  \n",
      "3         NaN    100000  \n",
      "4         NaN    250000  \n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_amount_controlled():\n",
    "\n",
    "    # d_prime = { ('amount' : {'date' : set(people_who_traded_on_that_day) , ....} , ....}\n",
    "    d_prime = {}\n",
    "\n",
    "    \n",
    "    for _,t in rows:\n",
    "       d_prime = dict_utils.increment_set_in_dictionary(d_prime, t[constants.AMOUNT], ptr_utils.format_date(t[constants.TDATE]), official.get_canonical_name(t[title]))\n",
    "       \n",
    "\n",
    "    for ticker in d_prime:\n",
    "        for date in d_prime[ticker]:\n",
    "            d_prime[ticker][date] = len(d_prime[ticker][date])\n",
    "            \n",
    "    \n",
    "    # d = {'amount' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "    d = dict_utils.add_sort_key_for_amount(d)\n",
    "    d = dict_utils.sort_dictionary_by_sort_key(d)\n",
    "    \n",
    "    \n",
    "    filename = \"most_popular_td_fe_amount_controlled\"\n",
    "    key_header = constants.AMOUNT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, key_header                                                                                                                                                                                                                                      )\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_amount_controlled_res = transaction_date_wrt_amount_controlled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular transaction_date for each official."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               senator transaction_date  number_of_transactions\n",
      "0      Loeffler, Kelly       2020/04/07                     111\n",
      "1  Perdue Jr., David A       2020/04/14                     110\n",
      "2     Tillis, Thomas R       2015/02/13                      93\n",
      "3        Murray, Patty       2017/06/15                      83\n",
      "4     Collins, Susan M       2014/05/07                      64\n"
     ]
    }
   ],
   "source": [
    "def transaction_date_wrt_official():\n",
    "\n",
    "    # d_prime = {'person1' : {'date' : #_of_transactions, ....} , 'person2' : .... }\n",
    "    d_prime = {}\n",
    "    \n",
    "    for _,t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.TICKER]):\n",
    "            d_prime = dict_utils.increment_dictionary_in_dictionary(d_prime, official.get_canonical_name(t[title]), ptr_utils.format_date(t[constants.TDATE]))\n",
    "       \n",
    "    # d = {'person' : {'best_date' : #_of_transactions}, .... }\n",
    "    d = dict_utils.flatten(d_prime)\n",
    "\n",
    "\n",
    "    d = dict_utils.sort_dictionary_by_inner_values(d, reverse=True)\n",
    "\n",
    "    filename = \"most_popular_td_fe_official\"\n",
    "    key_header = title\n",
    "    value_header = constants.TDATE\n",
    "    value_header2 = constants.NUMT\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header, value_header2])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "transaction_date_wrt_official_res = transaction_date_wrt_official()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average amount size of transactions (i.e., activity) for each transaction_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date average_size_of_transactions\n",
      "0  07/23/2021                   11,180,341\n",
      "1  07/22/2020                   11,180,341\n",
      "2  10/27/2020                    2,236,069\n",
      "3  01/23/2020                    2,236,069\n",
      "4  02/22/2019                    2,177,940\n"
     ]
    }
   ],
   "source": [
    "def dates_and_size_of_amount():\n",
    "\n",
    "    d={}\n",
    "    \n",
    "    for _, t in rows:\n",
    "        if ptr_utils.isvalid(t[constants.AMOUNT]):\n",
    "            \n",
    "            # if 'Purchase' in transaction['type']:\n",
    "                \n",
    "            # if 'Sale' in transaction['type']:\n",
    "            d = dict_utils.increment_dictionary_in_dictionary(d, t[constants.TDATE], ptr_utils.average_amount(t[constants.AMOUNT]))\n",
    "\n",
    "\n",
    "    d2 = {}\n",
    "    for date in d:\n",
    "        l = []\n",
    "        for amount in d[date]:   \n",
    "            l.append(d[date][amount]*amount)\n",
    "            \n",
    "        d2[date] = l \n",
    "        \n",
    "        \n",
    "    for date in d2:\n",
    "        d2[date] = int(gmean(d2[date]))\n",
    "    \n",
    "    filename = \"dates_and_size_of_amount\"\n",
    "    key_header = \"date\" \n",
    "    value_header = \"average_size_of_transactions\"\n",
    "\n",
    "    d2 = dict_utils.sort_dictionary_by_values(d2)\n",
    "    d2 = dict_utils.commify(d2)\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d2, [key_header, value_header])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "\n",
    "    return d2 \n",
    "\n",
    "dates_and_size_of_amount_res = dates_and_size_of_amount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Person by Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                senator  01/01/2014  01/02/2013  01/02/2014  01/02/2015  \\\n",
      "0  Tuberville, Thomas H         NaN         NaN         NaN         NaN   \n",
      "1     Lummis, Cynthia M         NaN         NaN         NaN         NaN   \n",
      "2      Carper, Thomas R         NaN         NaN         1.0         NaN   \n",
      "3            Blunt, Roy         NaN         NaN         NaN         NaN   \n",
      "4     Capito, Shelley M         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   01/02/2018  01/02/2020  01/03/2014  01/03/2017  01/03/2018  ...  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "1         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "2         NaN         2.0         1.0         NaN         NaN  ...   \n",
      "3         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "4         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "\n",
      "   12/30/2014  12/30/2015  12/30/2016  12/30/2019  12/30/2020  12/31/2014  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN         8.0         1.0   \n",
      "3         1.0         NaN         NaN         NaN         NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   12/31/2015  12/31/2018  12/31/2019  12/31/2020  \n",
      "0         NaN         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN         NaN  \n",
      "4         NaN         1.0         NaN         9.0  \n",
      "\n",
      "[5 rows x 1833 columns]\n"
     ]
    }
   ],
   "source": [
    "def num_of_trans_per_person_per_date():    \n",
    "\n",
    "    d={}\n",
    "\n",
    "    for _, t in rows:\n",
    "        d = dict_utils.increment_dictionary_in_dictionary(d, official.get_canonical_name(t[title]), t[constants.TDATE])\n",
    "\n",
    "    filename = \"num_of_trans_per_person_per_date\"\n",
    "    \n",
    "    \n",
    "    # some sort of error \n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv_breakdown(dir, filename, d, title)\n",
    "    print(pd.read_csv(wd).head(5))\n",
    "\n",
    "num_of_trans_per_person_per_date()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  number_of_transactions\n",
      "0  04/14/2020                     148\n",
      "1  04/07/2020                     116\n",
      "2  06/15/2017                     107\n",
      "3  02/13/2015                     100\n",
      "4  03/16/2017                      81\n"
     ]
    }
   ],
   "source": [
    "def num_of_trans_per_person_per_date():\n",
    "\n",
    "    d={}\n",
    "\n",
    "    for _, t in rows:\n",
    "        d = dict_utils.increment_dictionary(d, t[constants.TDATE])\n",
    "        \n",
    "\n",
    "    filename = \"num_of_trans_per_person_per_date\"\n",
    "    key_header = \"date\"\n",
    "    value_header = \"number_of_transactions\"\n",
    "    \n",
    "    d = dict_utils.sort_dictionary_by_values(d)\n",
    "    \n",
    "    dir = dir_utils.makesubdir(constants.path_csv, constants.TDATE)\n",
    "    wd = csv_utils.make_csv(dir, filename, d, [key_header, value_header])\n",
    "    # fig = graph_csv(dir, path_html, filename, key_header, value_header)\n",
    "    # fig.show()\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "\n",
    "trans_per_person_per_date_res = num_of_trans_per_person_per_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Transactions per Date Controlled\n",
    "_Number of transactions per date controlled by official. E.g. if Ted Baker made 40 transactions on 1/1/02 and Sam Wall made 2 transactions on 1/1/02, we conclude that there were two transactions on 1/1/02._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  number_of_transactions_unique\n",
      "0  08/27/2020                              6\n",
      "1  02/27/2019                              6\n",
      "2  10/25/2018                              6\n",
      "3  03/09/2020                              6\n",
      "4  11/19/2019                              6\n"
     ]
    }
   ],
   "source": [
    "def num_of_trans_per_date_controlled():\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    for _, t in rows:\n",
    "        d = dict_utils.increment_dictionary_in_dictionary(d, t[constants.TDATE], official.get_canonical_name(t[title]))\n",
    "    \n",
    "    d2 = {}\n",
    "    for date in d:\n",
    "        d2[date] =  len(d[date])\n",
    "        \n",
    "    d2 = dict_utils.sort_dictionary_by_values(d2)\n",
    "        \n",
    "    filename = \"num_of_trans_per_date_controlled\"\n",
    "    key_header = \"date\"\n",
    "    value_header = \"number_of_transactions_unique\"\n",
    "\n",
    "\n",
    "    dir = dir_utils.makesubdir(constants.path_csv, \"transaction_date\")\n",
    "    wd = csv_utils.make_csv(dir, filename, d2, [key_header, value_header])\n",
    "    # fig = graph_csv(dir, path_html, filename, key_header, value_header, scatter=True)\n",
    "    # fig.show()\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "    return d \n",
    "    \n",
    "num_of_trans_per_date_controlled_res = num_of_trans_per_date_controlled()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tax (not touched.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Transactions Within 2 Weeks Prior to Quarterly Tax Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import within_tax_date, increment_dictionary\n",
    "from search import wiki_search\n",
    "\n",
    "# i should like type of transactions @TODO. \n",
    "\n",
    "def num_of_trans_within_tax_date(rows):\n",
    "        total = within = 0 \n",
    "        \n",
    "        for k,v in rows.items():  \n",
    "                total += 1 \n",
    "                if within_tax_date(k):\n",
    "                        within += v \n",
    "\n",
    "        print(\"Percent of transactions posted within two weeks of quarterly tax deadline: {percent}%\".format(percent=str((within/total)*100)[:5]))\n",
    "        return (within/total)*100\n",
    "\n",
    "frac = num_of_trans_within_tax_date(trans_per_person_per_date_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Transactions Within 2 Weeks Prior to Quarterly Tax Date Semi-Controlled \n",
    "\n",
    "_Given dict='09/03/2021': {'Thomas H Tuberville': 1, 'Cynthia M Lummis': 1, 'A. Mitchell Mcconnell, Jr.': 1}...I only incremement the number of within (tax date) once per date per official. So, if an official does 100 transactions on a date within two weeks of a quarterly deadline, then I only count it as one transaction._\n",
    "\n",
    "_A Note: total === number of transactions per person per date (so not really all transactions) because someone could have potentially made 60 transactions on one date which we don't include in neither total or within, if applicable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_within_tax_date_controlled(rows):\n",
    "        total = within = 0 \n",
    "        people = set()\n",
    "\n",
    "        for date, inner_dict in rows.items():\n",
    "                if within_tax_date(date):\n",
    "                    for person in inner_dict:\n",
    "                                people.add(person)\n",
    "                                within += 1 \n",
    "                                total += 1 \n",
    "                else:\n",
    "                        for person in inner_dict:\n",
    "                                total += 1 \n",
    "     \n",
    "     \n",
    "        print(\"Percent of transactions posted within two weeks of quarterly tax deadline: {percent}%\".format(percent=str((within/total)*100)[:5]))\n",
    "        return people\n",
    "\n",
    "num_of_trans_within_tax_date_controlled_res = num_of_trans_within_tax_date_controlled(num_of_trans_per_date_controlled_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sort_dictionary_by_keys\n",
    "\n",
    "def people_and_within_tax_date(people):        \n",
    "        # todo get number of senators. \n",
    "        # todo is the monetary value of that equal!!!! \n",
    "        d = {}\n",
    "        for i in people:\n",
    "                d[i] = \"\"\n",
    "                \n",
    "        d = sort_dictionary_by_keys(d)\n",
    "        \n",
    "        dir = makesubdir(path_csv, \"transaction_date/tax\")\n",
    "        wd = make_csv(dir, \"people_and_within_tax_date_list\", d, [\"Officials\"])\n",
    "        df = pd.read_csv(wd)\n",
    "        print(\"People who posted transactions within two weeks of quarterly tax deadline:\\n {}\\n\".format(df.head(5)))\n",
    "\n",
    "        print(\"Number of people who posted transactions within two weeks of quarterly tax deadline: {}\\n\".format(len(people)))\n",
    "        \n",
    "        party = {}\n",
    "        for p in people:\n",
    "                party = increment_dictionary(party, wiki_search(p).get_party())\n",
    "                \n",
    "        party = sort_dictionary_by_values(party)\n",
    "        \n",
    "        wd = make_csv(dir, \"people_and_within_tax_date_list_w_aff\", party, [\"party\", \"number_of_filing_within_tax_date\"])\n",
    "        df = pd.read_csv(wd)\n",
    "        print(\"Party breakdown of people who posted transactions within two weeks of quarterly tax deadline:\\n {}\\n\".format(df.head(5)))\n",
    "\n",
    "people_and_within_tax_date(num_of_trans_within_tax_date_controlled_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_and_within_tax_date_how_often(people):\n",
    "\n",
    "        d = {}\n",
    "        d_controlled_by_dates = {}\n",
    "        \n",
    "        for _, transaction in rows:\n",
    "                if get_canonical_name(transaction[title]) in people and within_tax_date(transaction['transaction_date']):\n",
    "                        d = increment_dictionary(d, transaction[title])\n",
    "                        d_controlled_by_dates = increment_dictionary_in_dictionary(d_controlled_by_dates, transaction['transaction_date'], transaction[title])\n",
    "\n",
    "        d_controlled_by_dates_res  = {}\n",
    "        for date in d_controlled_by_dates:\n",
    "                for person in d_controlled_by_dates[date]:\n",
    "                        d_controlled_by_dates_res = increment_dictionary(d_controlled_by_dates_res, person)\n",
    "\n",
    "        d = sort_dictionary_by_values(d)\n",
    "        d_controlled_by_dates_res = sort_dictionary_by_values(d_controlled_by_dates_res)\n",
    "\n",
    "        dir = makesubdir(path_csv, \"transaction_date/tax\")\n",
    "        wd = make_csv(dir, \"people_and_within_tax_date_how_often\", d, [title, \"number_of_filing_within_tax_date\"])\n",
    "        df = pd.read_csv(wd)\n",
    "        print(\"People who posted transactions within two weeks of quarterly tax deadline and the number of transactions posted:\\n {}\\n\".format(df.head(5)))\n",
    "\n",
    "        wd = make_csv(dir, \"people_and_within_tax_date_how_often_date_controlled\", d_controlled_by_dates_res, [title, \"number_of_filing_within_tax_date_date_controlled\"])\n",
    "        df = pd.read_csv(wd)\n",
    "        print(\"People who posted transactions within two weeks of quarterly tax deadline and the number of transactions posted controlled by date:\\n {}\\n\".format(df.head(5)))\n",
    "\n",
    "          \n",
    "people_and_within_tax_date_how_often(num_of_trans_within_tax_date_controlled_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owner (owner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticker (ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Ticker per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_ticker_breakdown_ticker():\n",
    "    d = {}\n",
    "\n",
    "\n",
    "    for _, transaction in rows:\n",
    "        if isvalid(transaction['ticker']):\n",
    "            d = increment_dictionary_in_dictionary(d, transaction['ticker'], get_year(transaction['transaction_date']))\n",
    "            d = increment_dictionary_in_dictionary(d, transaction['ticker'], \"Total\")\n",
    "\n",
    "    d = sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"trans_per_year_breakdown\"\n",
    "    key_header = \"ticker\"\n",
    "    \n",
    "    \n",
    "    dir = makesubdir(path_csv, \"ticker\")\n",
    "    wd = make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "\n",
    "frequency_of_ticker_breakdown_ticker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Ticker per Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_year, increment_dictionary_in_dictionary, path_csv\n",
    "from csv_utils import make_csv_breakdown\n",
    "\n",
    "def frequency_of_ticker_by_date():\n",
    "    d = {}\n",
    "\n",
    "    \n",
    "    for _, transaction in rows:\n",
    "        if isvalid(transaction['ticker']):\n",
    "            d = increment_dictionary_in_dictionary(d, transaction['ticker'], transaction['transaction_date'])\n",
    "\n",
    "    filename = \"frequency_of_ticker_by_date\"\n",
    "    key_header = \"ticker\"\n",
    "    \n",
    "    dir = makesubdir(path_csv, \"ticker\")\n",
    "    wd = make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "\n",
    "frequency_of_ticker_by_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Transactions per Industry\n",
    "_Not controlled in any way._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_transactions_per_indusry():        \n",
    "    d = {}\n",
    "\n",
    "    df = get_mapping()\n",
    "    \n",
    "    for _, transaction in rows:\n",
    "        ticker = transaction['ticker']\n",
    "        industry = search_mapping(df, ticker)\n",
    "        if industry: \n",
    "            d = increment_dictionary(d, industry)\n",
    "\n",
    "    filename = \"number_of_transactions_per_indusry\"\n",
    "    key_header = \"industry\"\n",
    "    value_header = \"number_of_transactions\"\n",
    "\n",
    "    d = sort_dictionary_by_values(d)\n",
    "    \n",
    "    dir = makesubdir(path_csv, \"ticker/industry\")\n",
    "    wd = make_csv(dir, filename, d, [key_header, value_header])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))    \n",
    "    \n",
    "    return d \n",
    "\n",
    "number_of_transactions_per_indusry_res = number_of_transactions_per_indusry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industry Breakdown per Official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_industry_breakdown_official():\n",
    "    d = {}\n",
    "\n",
    "    df = get_mapping()\n",
    "\n",
    "    for _, transaction in rows:\n",
    "        industry = search_mapping(df, transaction['ticker'])\n",
    "        if industry: \n",
    "            d = increment_dictionary_in_dictionary(d, get_canonical_name(transaction[title]), industry)\n",
    "\n",
    "    d = sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"frequency_of_industry_per_official\"\n",
    "    dir = makesubdir(path_csv, \"ticker/industry\")\n",
    "\n",
    "    wd = make_csv_breakdown(dir, filename, d, title)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "\n",
    "frequency_of_industry_breakdown_official()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency of Industry per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_industry_breakdown():\n",
    "    \n",
    "    d = {}\n",
    "    df = get_mapping()\n",
    "\n",
    "    for _, transaction in rows:\n",
    "        ticker = transaction['ticker']\n",
    "        \n",
    "        if isvalid(ticker):\n",
    "            industry = search_mapping(df, ticker)\n",
    "            if industry: \n",
    "                d = increment_dictionary_in_dictionary(d, industry, get_year(transaction['transaction_date']))\n",
    "\n",
    "    d = sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"frequency_of_industry_breakdown\"\n",
    "    key_header = \"industry\"\n",
    "    \n",
    "    \n",
    "    dir = makesubdir(path_csv, \"ticker/industry\")\n",
    "    wd = make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "\n",
    "frequency_of_industry_breakdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asset Description (asset_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asset Type (asset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Asset Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_asset_type():\n",
    "    d = {}\n",
    "\n",
    "\n",
    "    for _, transaction in rows:\n",
    "        if isvalid(transaction['asset_type']):\n",
    "            d = increment_dictionary(d, transaction['asset_type'])\n",
    "      \n",
    "    d = sort_dictionary_by_values(d)\n",
    "  \n",
    "    filename = \"frequency_of_asset_type\"\n",
    "    key_header = \"asset_type\"\n",
    "    value_header = \"number_of_transactions\"\n",
    "    \n",
    "    dir = makesubdir(path_csv, \"asset_type\")\n",
    "    wd = make_csv(dir, filename, d, [key_header, value_header])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "\n",
    "\n",
    "frequency_of_asset_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Amount by Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_amount_by_persom():\n",
    "    d = {}\n",
    "\n",
    "\n",
    "    for _, transaction in rows:\n",
    "        d = increment_dictionary_in_dictionary(d, get_canonical_name(transaction[title]), transaction['amount'])\n",
    "\n",
    "    \n",
    "    d = sort_dictionary_by_keys(d)\n",
    "    \n",
    "    filename = \"frequency_of_amount_by_persom\"\n",
    "    key_header = \"amount\"\n",
    "    \n",
    "    dir = makesubdir(path_csv, \"amount\")\n",
    "    wd = make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "   \n",
    "    \n",
    "frequency_of_amount_by_persom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Transactions by Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_amount_total():\n",
    "    d = {}\n",
    "\n",
    "\n",
    "    for _, transaction in rows:\n",
    "        d = increment_dictionary(d, transaction['amount'])\n",
    "\n",
    "    d = add_sort_key_for_amount(d, normal_header=\"num_of_transactions\", normal=True)\n",
    "    d = sort_dictionary_by_sort_key(d)\n",
    "\n",
    "    filename = \"frequency_of_amount_total\"\n",
    "    key_header = \"amount\"\n",
    "\n",
    "\n",
    "    dir = makesubdir(path_csv, \"amount\")\n",
    "    wd = make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "   \n",
    "    \n",
    "frequency_of_amount_total()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Amount by Gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_amount_by_gender():\n",
    "    d = {}\n",
    "\n",
    "\n",
    "    gender = {}\n",
    "    for _, transaction in rows:\n",
    "        person = transaction[title]\n",
    "        \n",
    "        if person not in gender: \n",
    "            rep = wiki_search(person) \n",
    "            gender.update({person : rep.get_gender()})\n",
    "            \n",
    "        d = increment_dictionary_in_dictionary(d, transaction['amount'], gender[person])\n",
    "        \n",
    "\n",
    "    d = add_sort_key_for_amount(d)\n",
    "    d = sort_dictionary_by_sort_key(d)\n",
    "    \n",
    "        \n",
    "    filename = \"frequency_of_amount_by_gender\"\n",
    "    key_header = \"amount\"\n",
    "    \n",
    "    \n",
    "    dir = makesubdir(path_csv, \"amount\")\n",
    "    wd = make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "   \n",
    "    \n",
    "frequency_of_amount_by_gender()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Transactions by Political Affiliation and Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def frequency_of_amount_by_aff():\n",
    "    d = {}\n",
    "\n",
    "\n",
    "    affiliations = {}\n",
    "    for _, transaction in rows:\n",
    "        person = transaction[title]\n",
    "        \n",
    "        if person not in affiliations: \n",
    "            rep = wiki_search(person) \n",
    "            affiliations.update({person : rep.party})\n",
    "            \n",
    "        d = increment_dictionary_in_dictionary(d, transaction['amount'], affiliations[person])\n",
    "\n",
    "\n",
    "    d = add_sort_key_for_amount(d)\n",
    "    d = sort_dictionary_by_sort_key(d)\n",
    "    \n",
    "        \n",
    "    filename = \"frequency_of_amount_by_aff\"\n",
    "    key_header = \"amount\"\n",
    "    \n",
    "    \n",
    "    dir = makesubdir(path_csv, \"amount\")\n",
    "    wd = make_csv_breakdown(dir, filename, d, key_header)\n",
    "    print(pd.read_csv(wd).head(2))\n",
    "   \n",
    "    \n",
    "frequency_of_amount_by_aff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average For Buys and Sells per Official "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_per_person():\n",
    "\n",
    "    d={}\n",
    "    \n",
    "    for _, transaction in rows:\n",
    "        if isvalid(transaction['amount']):\n",
    "            \n",
    "            # if 'Purchase' in transaction['type']:\n",
    "                \n",
    "            # if 'Sale' in transaction['type']:\n",
    "            d = increment_dictionary_in_dictionary(d, get_canonical_name(transaction[title]), average_amount(transaction['amount']))\n",
    "\n",
    "\n",
    "    d2 = {}\n",
    "    for person in d:\n",
    "        l = []\n",
    "        for amount in d[person]:   \n",
    "            l.append(d[person][amount]*amount)\n",
    "            \n",
    "        d2[person] = l \n",
    "        \n",
    "        \n",
    "    for person in d2:\n",
    "        d2[person] = int(gmean(d2[person]))\n",
    "    \n",
    "    filename = \"average_per_person\"\n",
    "    key_header = title \n",
    "    value_header = \"average_size_of_transactions\"\n",
    "\n",
    "    d2 = sort_dictionary_by_values(d2)\n",
    "    d2 = commify(d2)\n",
    "\n",
    "    dir = makesubdir(path_csv, \"amount\")\n",
    "    wd = make_csv(dir, filename, d2, [key_header, value_header])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "\n",
    "    return d2 \n",
    "\n",
    "average_per_person_res = average_per_person()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types (type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Actions Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_act():\n",
    "    d = {}\n",
    "\n",
    "    for _, transaction in rows:\n",
    "        if isvalid(transaction['type']): \n",
    "            d = increment_dictionary(d, transaction['type'])\n",
    "    \n",
    "    d = sort_dictionary_by_values(d)\n",
    "    \n",
    "    filename = \"frequency_of_act\"\n",
    "    key_header = \"type\"\n",
    "    value_header = \"number_of_transactions\"\n",
    "    \n",
    "    dir = makesubdir(path_csv, \"type\")\n",
    "    wd = make_csv(dir, filename, d, [key_header, value_header])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "\n",
    "     \n",
    "frequency_of_act()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Transactions per Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import increment_dictionary_in_dictionary, sort_dictionary_by_keys, get_data, path_csv,makesubdir\n",
    "from csv_utils import make_csv_breakdown\n",
    "import pandas as pd \n",
    "from official import get_canonical_name\n",
    "\n",
    "\n",
    "def types_of_transactions_per_person():\n",
    "\n",
    "    d={}\n",
    "    \n",
    "    for _, transaction in rows:\n",
    "        d = increment_dictionary_in_dictionary(d, get_canonical_name(transaction[title]), transaction['type'])\n",
    "\n",
    "\n",
    "    filename = \"types_of_transactions_per_person\"\n",
    "\n",
    "    d = sort_dictionary_by_keys(d)\n",
    "\n",
    "    dir = makesubdir(path_csv, \"type\")\n",
    "    wd = make_csv_breakdown(dir, filename, d, title)\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "\n",
    "    return d \n",
    "\n",
    "types_of_transactions_per_person_res = types_of_transactions_per_person()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment (comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_of_trans_per_year():\n",
    "    d={}\n",
    "    \n",
    "    for _, transaction in rows:\n",
    "        d = increment_dictionary(d, get_year(transaction['transaction_date']))\n",
    "\n",
    "    filename = \"num_of_trans_per_year\"\n",
    "    key_header = \"year\"\n",
    "    value_header = \"number_of_transactions\"\n",
    "\n",
    "    d = sort_dictionary_by_values(d)\n",
    "    \n",
    "    dir = makesubdir(path_csv, \"frequency\")\n",
    "    wd = make_csv(dir, filename, d, [key_header, value_header])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "\n",
    "    return d \n",
    "\n",
    "num_of_trans_per_year_res = num_of_trans_per_year()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Person \n",
    "_Not controlling for number of years in position or size of transaction._ For each official, we want their total number of transactions.  {'Sam': 5, 'Alex': 2424, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_of_trans_per_person():\n",
    "\n",
    "    d={}\n",
    "    \n",
    "    for _, transaction in rows:\n",
    "        d = increment_dictionary(d, get_canonical_name(transaction[title]))\n",
    "\n",
    "    filename = \"num_of_trans_per_person\"\n",
    "    key_header = title\n",
    "    value_header = \"number_of_transactions\"\n",
    "\n",
    "    d = sort_dictionary_by_values(d)\n",
    "    \n",
    "    dir = makesubdir(path_csv, \"frequency\")\n",
    "    wd = make_csv(dir, filename, d, [key_header, value_header])\n",
    "    df = pd.read_csv(wd)\n",
    "    print(df.head(5))\n",
    "\n",
    "    return d \n",
    "\n",
    "trans_per_person_res = num_of_trans_per_person()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Person Controlled\n",
    "_Divide number of transactions by number of years in official position.  Not controlling for size of transaction._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_per_person_controlled(rows):    \n",
    "\n",
    "    d={}\n",
    "\n",
    "    for person, num_of_trans in rows.items():\n",
    "        official = wiki_search(person)    \n",
    "        years_in_office = official.get_num_of_years()\n",
    "        d[person] = math.ceil(num_of_trans/years_in_office) \n",
    "\n",
    "    filename = \"num_of_trans_per_person_controlled\"\n",
    "    value_header = \"avg_number_of_transactions\"\n",
    "\n",
    "    d = sort_dictionary_by_values(d)\n",
    "   \n",
    "    dir = makesubdir(path_csv, \"frequency\")\n",
    "    wd = make_csv(dir, filename, d, [title, value_header])\n",
    "    print(pd.read_csv(wd).head(5))\n",
    "    \n",
    "    return d \n",
    "\n",
    "num_of_trans_per_person_controlled_res = num_of_trans_per_person_controlled(trans_per_person_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Person Controlled w/Number of Years in Congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_per_person_controlled_w_seniority(rows):    \n",
    "\n",
    "    d={}\n",
    "\n",
    "    for person, num_of_trans in rows.items():\n",
    "        official = wiki_search(person)    \n",
    "        years_in_office = official.get_num_of_years()\n",
    "        d[person] = math.ceil(num_of_trans/years_in_office), official.get_num_of_years()\n",
    "\n",
    "    filename = \"num_of_trans_per_person_controlled_w_seniority\"\n",
    "    value_header = \"avg_number_of_transactions\"\n",
    "\n",
    "    d = sort_dictionary_by_values(d)\n",
    "   \n",
    "    dir = makesubdir(path_csv, \"frequency\")\n",
    "    wd = make_csv(dir, filename, d, [title, value_header, \"seniority\"])\n",
    "    print(pd.read_csv(wd).head(5))\n",
    "    \n",
    "    return d \n",
    "\n",
    "num_of_trans_per_person_controlled_w_seniority_res = num_of_trans_per_person_controlled_w_seniority(trans_per_person_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Person Controlled w/Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_trans_per_person_controlled_w_degrees(rows):    \n",
    "\n",
    "\n",
    "    d={}\n",
    "\n",
    "    for person, num_of_trans in rows.items():\n",
    "        official = wiki_search(person)    \n",
    "        years_in_office = official.get_num_of_years()\n",
    "        d[person] = math.ceil(num_of_trans/years_in_office), official.get_num_of_degrees()\n",
    "\n",
    "    filename = \"num_of_trans_per_person_controlled_w_degrees\"\n",
    "    value_header = \"avg_number_of_transactions\"\n",
    "\n",
    "    d = sort_dictionary_by_values(d)\n",
    "   \n",
    "    dir = makesubdir(path_csv, \"frequency\")\n",
    "    wd = make_csv(dir, filename, d, [title, value_header, \"num_of_degrees\"])\n",
    "    print(pd.read_csv(wd).head(5))\n",
    "    \n",
    "    return d \n",
    "\n",
    "num_of_trans_per_person_controlled_w_degrees_res = num_of_trans_per_person_controlled_w_degrees(trans_per_person_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Transactions per Person Controlled w/Affiliation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_of_trans_per_person_controlled_w_aff(rows):\n",
    "\n",
    "    d={}\n",
    "    \n",
    "    for person, val in rows.items():\n",
    "        official = wiki_search(person)     \n",
    "        d[person] = val, official.get_party()\n",
    "\n",
    "    filename = \"num_of_trans_per_person_controlled_w_aff\"\n",
    "    value_header = \"avg_number_of_transactions\"\n",
    "\n",
    "    d = sort_dictionary_by_values(d)\n",
    "        \n",
    "    dir = makesubdir(path_csv, \"frequency\")\n",
    "    wd = make_csv(dir, filename, d, [title, value_header, \"party\"])\n",
    "    print(pd.read_csv(wd).head(5))\n",
    "    \n",
    "    return d, pd.read_csv(wd).head(10)\n",
    "\n",
    "num_of_trans_per_person_controlled_w_aff_res, top_10 = num_of_trans_per_person_controlled_w_aff(num_of_trans_per_person_controlled_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Date (transaction_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency of Differences between Transaction and Disclosure Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_differences():\n",
    "    d = {}\n",
    "    match = {}\n",
    "    total = num = 0 \n",
    "\n",
    "    for _, transaction in rows:\n",
    "        transaction_date = transaction['transaction_date']\n",
    "        disclosure_date = transaction['disclosure_date']\n",
    "\n",
    "        # Negative, X days BEFORE\n",
    "        # Positive, Y dayas AFTER\n",
    "        diff = difference_between_dates(disclosure_date, transaction_date)      \n",
    "        total += 1 \n",
    "        num += diff   \n",
    "        \n",
    "        # match = {5 days: {'Tom' : 1313, 'X': 3 , ...}. ..}\n",
    "        match = increment_dictionary_in_dictionary(match, diff, transaction[title])\n",
    "            \n",
    "            \n",
    "        d = increment_dictionary(d, int(diff))\n",
    "    \n",
    "    d = sort_dictionary_by_keys(d)\n",
    "        \n",
    "    filename = \"frequency_of_differences\"\n",
    "    key_header = \"difference_in_days\"\n",
    "    value_header = \"#_of_transactions_with_that_diff\"\n",
    "    \n",
    "    \n",
    "    dir = makesubdir(path_csv, \"disclosure_date/transaction_date\")\n",
    "    make_csv(dir, filename, d, [key_header, value_header])\n",
    "    # fig = graph_csv(dir, path_html, filename, key_header, value_header)\n",
    "    # fig.show()\n",
    "    \n",
    "    print(\"Average difference in days: {}\".format(num//total))\n",
    "\n",
    "frequency_of_differences()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
